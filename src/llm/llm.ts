// src/llm/llm.ts
import { ProviderAdapter } from '../providers/adapter/providerAdapter';
import type { Message } from '../memory';
import type { IProviderResponse } from '../providers/adapter/providerAdapter.interface';
import type { ProviderConfig } from '../providers/adapter/providerAdapter.interface';
import { PromptBuilder } from '../promptBuilder';
import type { PromptBuilderConfig, PromptMode, AgentInfo, ToolSchema } from '../promptBuilder';

/**
 * Par√¢metros padr√£o por provedor (aplicados quando n√£o informados na chamada).
 *
 * - temperature: temperatura do modelo (default sugerido 0.5)
 * - topP: nucleus sampling
 * - maxTokens: limite de tokens de sa√≠da
 */
interface ProviderDefaults {
  temperature?: number;
  topP?: number;
  maxTokens?: number;
}

/**
 * Cliente LLM baseado no ProviderAdapter.
 * Mant√©m `model` e `apiKey` fixos e aplica op√ß√µes a cada chamada.
 *
 * Use `invoke` com `mode` e `agentInfo` obrigat√≥rios para gerar o systemPrompt
 * via PromptBuilder internamente (ex.: modo 'react', 'chat', etc.).
 */
export class LLM {
  private readonly model: string;
  private readonly apiKey: string;
  private readonly defaults: ProviderDefaults;
  private readonly baseUrl?: string;

  /**
   * Cria uma inst√¢ncia de LLM com modelo/chave fixos.
   * @param params.model Modelo completo (ex.: 'openaiCompatible-gpt-4o-mini' ou 'openai-gpt-4o')
   * @param params.apiKey Chave do provedor escolhido
   * @param params.defaults Valores padr√£o (temperature/topP/maxTokens)
   */
  constructor(params: { model: string; apiKey: string; defaults?: ProviderDefaults; baseUrl?: string }) {
    this.model = params.model;
    this.apiKey = params.apiKey;
    this.defaults = params.defaults ?? {};
    this.baseUrl = params.baseUrl;
    
    console.log(`[LLM] LLM instance created with model: ${this.model}`);
  }

  /**
   * Garante que o modo esteja registrado no PromptBuilder, sen√£o lan√ßa erro amig√°vel.
   * @private
   */
  private assertModeRegistered(mode: PromptMode): void {
    try {
      // Minimal config to trigger builder existence
      PromptBuilder.buildSystemPrompt({
        mode,
        agentInfo: { name: 'validator', goal: 'validate mode', backstory: '' },
      } as unknown as PromptBuilderConfig);
    } catch (e) {
      throw new Error(
        `Prompt mode '${mode}' n√£o est√° registrado. Importe o m√≥dulo correspondente (ex.: 'src/agents') antes de invocar.`
      );
    }
  }

  /**
   * Invoca o provedor configurado com modo e informa√ß√µes do agente obrigat√≥rios.
   * O systemPrompt √© gerado internamente via PromptBuilder a partir do modo e agentInfo.
   *
   * @param args Par√¢metros de invoca√ß√£o com mode e agentInfo obrigat√≥rios
   * @returns Conte√∫do textual e metadados do provedor (quando dispon√≠veis)
   */
  public async invoke(args: {
    messages: Message[];
    mode?: PromptMode;
    agentInfo?: AgentInfo;
    systemPrompt?: string;
    additionalInstructions?: string;
    tools?: ToolSchema[];
    taskList?: { items: Array<{ id: string; title: string; status: 'pending' | 'in_progress' | 'completed' }> };
    temperature?: number;
    topP?: number;
    maxTokens?: number;
    stream?: boolean;
    promptConfig?: PromptBuilderConfig;
  }): Promise<{ content: string | null; metadata?: Record<string, unknown> }> {
    
    // üéØ LOG DIRETO DO INVOCATION - MOMENTO EXATO ANTES DE CONSTRUIR O PROMPT
    console.log('\n' + 'üß†' + '='.repeat(78));
    console.log('üéØ LLM.INVOKE - MOMENTO EXATO ANTES DE CONSTRUIR O PROMPT');
    console.log('='.repeat(80));
    console.log(`üìä N√∫mero de mensagens: ${args.messages.length}`);
    console.log(`üîß Modo: ${args.mode || 'n√£o definido'}`);
    console.log(`ü§ñ Agente: ${args.agentInfo?.name || 'n√£o definido'}`);
    console.log(`üå°Ô∏è  Temperatura: ${args.temperature || 'default'}`);
    console.log(`üî¢ Max Tokens: ${args.maxTokens || 'default'}`);
    
    if (args.messages && args.messages.length > 0) {
      console.log(`\nüí¨ MENSAGENS DO USU√ÅRIO:`);
      console.log('-'.repeat(60));
      args.messages.forEach((msg, index) => {
        console.log(`\n[${index}] Role: ${msg.role.toUpperCase()}`);
        console.log(`    Content (${msg.content?.length || 0} caracteres):`);
        if (msg.content) {
          console.log('    ' + msg.content.split('\n').join('\n    '));
        }
      });
      console.log('-'.repeat(60));
    }
    console.log('='.repeat(80) + '\n');
    
    console.log(`LLM.invoke called with ${args.messages.length} messages`, 'LLM');
    
    // Determina qual systemPrompt usar
    let systemPrompt: string;
    let promptSource: 'promptConfig' | 'systemPrompt' | 'mode+agentInfo+additionalInstructions';
    
    if (args.promptConfig) {
      // Usa promptConfig se fornecido
      this.assertModeRegistered(args.promptConfig.mode);
      systemPrompt = PromptBuilder.buildSystemPrompt(args.promptConfig);
      promptSource = 'promptConfig';
    } else if (args.systemPrompt) {
      // Usa systemPrompt direto se fornecido
      systemPrompt = args.systemPrompt;
      promptSource = 'systemPrompt';
    } else if (args.mode && args.agentInfo) {
      // Fallback para modo e agentInfo
      this.assertModeRegistered(args.mode);
      const promptConfig: PromptBuilderConfig = {
        mode: args.mode,
        agentInfo: args.agentInfo,
        additionalInstructions: args.additionalInstructions,
        tools: args.tools,
        taskList: args.taskList,
      };
      systemPrompt = PromptBuilder.buildSystemPrompt(promptConfig);
      promptSource = 'mode+agentInfo+additionalInstructions';
    } else {
      throw new Error('Deve fornecer promptConfig, systemPrompt, ou mode+agentInfo');
    }
    
    // üéØ LOG DO SYSTEM PROMPT COMPLETO AP√ìS CONSTRU√á√ÉO
    console.log('\n' + 'üìã' + '='.repeat(78));
    console.log('üìã SYSTEM PROMPT CONSTRU√çDO - CONTE√öDO COMPLETO');
    console.log('='.repeat(80));
    console.log(`üìä Fonte: ${promptSource}`);
    console.log(`üìè Tamanho: ${systemPrompt.length} caracteres`);
    console.log(`üî§ Preview: ${systemPrompt.length > 200 ? systemPrompt.substring(0, 200) + '...' : systemPrompt}`);
    console.log('\nüìÑ CONTE√öDO COMPLETO DO SYSTEM PROMPT:');
    console.log('-'.repeat(60));
    console.log(systemPrompt);
    console.log('-'.repeat(60));
    console.log('='.repeat(80) + '\n');
    
    const spPreview = systemPrompt.length > 1000 ? `${systemPrompt.slice(0, 1000)}...` : systemPrompt;
    console.log(`LLM SystemPrompt | source=${promptSource} | length=${systemPrompt.length}`, 'LLM');
    console.log(`LLM SystemPrompt preview: ${spPreview}`, 'LLM');
    
    // üéØ LOG ADICIONAL PARA VALIDAR TRUNCAMENTO NO LLM
    console.log('\n' + 'üîç' + '='.repeat(78));
    console.log('üîç LLM - VALIDA√á√ÉO DE TRUNCAMENTO DO SYSTEM PROMPT');
    console.log('='.repeat(80));
    console.log(`üìä Tamanho original: ${systemPrompt.length} caracteres`);
    console.log(`üìä Tamanho do preview: ${spPreview.length} caracteres`);
    console.log(`üîç Preview truncado? ${systemPrompt.length > 1000 ? 'SIM' : 'N√ÉO'}`);
    console.log(`üìã Preview: "${spPreview}"`);
    console.log('='.repeat(80) + '\n');
    const temperature = args.temperature ?? this.defaults.temperature ?? 0.5;
    const topP = args.topP ?? this.defaults.topP;
    const maxTokens = args.maxTokens ?? this.defaults.maxTokens;
    const stream = args.stream ?? false;

    const config: ProviderConfig = {
      model: this.model,
      apiKey: this.apiKey,
      messages: args.messages,
      systemPrompt,
      temperature,
      stream,
      topP,
      maxTokens,
      baseUrl: this.baseUrl,
    };

    console.log(`Calling ProviderAdapter with model: ${config.model}`, 'LLM');
    const resp: IProviderResponse = await ProviderAdapter.chatCompletion(config);
    console.log(`ProviderAdapter response received`, 'LLM');
    return { content: resp?.content ?? null, metadata: resp?.metadata };
  }
}

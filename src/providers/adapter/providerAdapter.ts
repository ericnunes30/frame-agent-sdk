import { ProviderConfig } from './providerAdapter.interface';
import { getProvider } from '../providers';
import { logger } from '@/utils/logger';
import { emitTrace } from '@/telemetry/utils/traceEmitter';
import { createTraceId } from '@/telemetry/utils/id';

/**
 * Adaptador gen√©rico unificado para provedores de LLM.
 * 
 * Esta classe fornece uma interface consistente para interagir com diferentes
 * provedores de modelos de linguagem, abstraindo as diferen√ßas entre APIs
 * e permitindo troca f√°cil entre provedores.
 * 
 * ## Funcionalidades Principais
 * 
 * - **Interface Unificada**: API consistente independente do provedor
 * - **Auto-detec√ß√£o**: Detecta automaticamente o provedor baseado no modelo
 * - **Fallback Inteligente**: Usa openaiCompatible para modelos desconhecidos com baseUrl
 * - **Logging Detalhado**: Log completo do prompt para debugging
 * - **Configura√ß√£o Flex√≠vel**: Suporte a todos os par√¢metros via ProviderConfig
 * 
 * ## Auto-detec√ß√£o de Provedores
 * 
 * O ProviderAdapter infere o provedor baseado no prefixo do modelo:
 * - `gpt-4` ‚Üí OpenAI
 * `claude-3-sonnet` ‚Üí OpenAI Compatible (se baseUrl fornecida)
 * - `meta-llama/llama-3.1-70b` ‚Üí OpenAI Compatible (se baseUrl fornecida)
 * 
 * @example
 * ```typescript
 * // Uso b√°sico
 * const config: ProviderConfig = {
 *   model: 'gpt-4',
 *   messages: [{ role: 'user', content: 'Ol√°!' }],
 *   apiKey: 'sua-api-key'
 * };
 * 
 * const response = await ProviderAdapter.chatCompletion(config);
 * console.log(response.content);
 * 
 * // Com provedor compat√≠vel
 * const compatibleConfig: ProviderConfig = {
 *   model: 'claude-3-sonnet',
 *   messages: [...],
 *   apiKey: 'sua-api-key',
 *   baseUrl: 'https://api.anthropic.com'
 * };
 * 
 * const response2 = await ProviderAdapter.chatCompletion(compatibleConfig);
 * ```
 * 
 * @see {@link ProviderConfig} Para configura√ß√£o completa
 * @see {@link ProviderRegistry} Para registro de provedores
 */
export class ProviderAdapter {
  /** 
   * Executa uma chamada de chat completion usando o provedor apropriado.
   * 
   * Este m√©todo √© o ponto de entrada principal do ProviderAdapter. Ele:
   * 1. Infere o provedor baseado no modelo
   * 2. Aplica defaults para par√¢metros n√£o especificados
   * 3. Implementa fallback para provedores compat√≠veis
   * 4. Loga informa√ß√µes detalhadas para debugging
   * 5. Chama o provedor apropriado com configura√ß√£o unificada
   * 
   * @param config Configura√ß√£o completa do provedor.
   * Deve incluir model, messages, apiKey e pode incluir par√¢metros opcionais.
   * 
   * @returns Resposta do provedor no formato unificado.
   * 
   * @throws {Error} Se o provedor n√£o estiver registrado
   * @throws {Error} Se o provedor n√£o implementar chatCompletion
   * @throws {Error} Se baseUrl for necess√°ria mas n√£o fornecida
   * 
   * @example
   * ```typescript
   * // Configura√ß√£o b√°sica
   * const response = await ProviderAdapter.chatCompletion({
   *   model: 'gpt-4',
   *   messages: [{ role: 'user', content: 'Explique IA' }],
   *   apiKey: 'sk-...',
   *   temperature: 0.7,
   *   maxTokens: 1000
   * });
   * 
   * // Com streaming
   * const streamingConfig = {
   *   model: 'gpt-4',
   *   messages: [...],
   *   apiKey: 'sk-...',
   *   stream: true
   * };
   * 
   * const streamResponse = await ProviderAdapter.chatCompletion(streamingConfig);
   * for await (const chunk of streamResponse) {
   *   process.stdout.write(chunk.content);
   * }
   * ```
   * 
   * @remarks
   * - O nome do provedor √© extra√≠do do prefixo do modelo (antes do primeiro '-')
   * - Par√¢metros n√£o especificados usam defaults inteligentes
   * - Modelos desconhecidos com baseUrl s√£o tratados como openaiCompatible
   * - Log detalhado √© emitido para debugging (pode ser desabilitado em produ√ß√£o)
   * 
   * @see {@link ProviderConfig} Para formato da configura√ß√£o
   */
  static async chatCompletion(config: ProviderConfig): Promise<any> {
    logger.info(`[ProviderAdapter] Iniciando chatCompletion com modelo: ${config.model}`);

    // Determinar provedor: usar expl√≠cito se dispon√≠vel, sen√£o inferir do modelo
    let providerName = config.provider || config.model.split('-')[0];
    logger.debug(`[ProviderAdapter] Provedor determinado: ${providerName} (Expl√≠cito: ${!!config.provider})`);

    // Aplicar defaults para par√¢metros n√£o especificados
    config.temperature = config.temperature ?? 0.7; // Default comum
    config.maxTokens = config.maxTokens ?? 2048;    // Default comum

    // Fallback inteligente: se provedor n√£o existe mas temos baseUrl,
    // assumir provedor compat√≠vel com OpenAI (ex: OpenRouter, Claude, etc.)
    if (!ProviderAdapter.hasProvider(providerName) && config.baseUrl) {
      logger.info(`[ProviderAdapter] Provedor espec√≠fico ${providerName} n√£o encontrado. Utilizando fallback 'openaiCompatible' (baseUrl: ${config.baseUrl})`);
      providerName = 'openaiCompatible';
    }

    // Log detalhado para debugging
    if (config.telemetry?.includePrompts && config.telemetry?.level === 'debug') {
      ProviderAdapter._logPromptDetails(providerName, config);
    }

    // Obter e instanciar o provedor
    logger.debug(`[ProviderAdapter] Obtendo provedor: ${providerName}`);
    const ProviderClass: any = getProvider(providerName);
    logger.debug(`[ProviderAdapter] Criando inst√¢ncia do provedor`);
    const provider = new ProviderClass(config.apiKey);

    // Extrair nome do modelo sem o prefixo do provedor
    const model = config.model.startsWith(providerName + '-')
      ? config.model.slice(providerName.length + 1)
      : config.model;
    logger.debug(`[ProviderAdapter] Modelo final: ${model}`);

    // Validar que o provedor implementa o m√©todo necess√°rio
    if (typeof provider.chatCompletion !== 'function') {
      throw new Error(`Provedor para o modelo ${config.model} n√£o implementa o m√©todo chatCompletion`);
    }

    // Chamar o provedor com configura√ß√£o unificada
    logger.debug(`[ProviderAdapter] Chamando chatCompletion do provedor`);

    const traceContext = config.traceContext;
    const spanId = createTraceId();
    const startedAt = Date.now();
    if (traceContext) {
      emitTrace({
        trace: config.trace,
        options: config.telemetry,
        ctx: traceContext,
        event: {
          type: 'llm_request_started',
          level: 'info',
          spanId,
          llm: { provider: providerName, model: config.model, stream: Boolean(config.stream) },
          data: { temperature: config.temperature, maxTokens: config.maxTokens, topP: config.topP },
        },
      });
    }
    try {
      const result = await provider.chatCompletion({ ...config, model });
      logger.debug(`[ProviderAdapter] chatCompletion conclu√≠do com sucesso`);
      if (traceContext) {
        const md = (result as any)?.metadata as Record<string, unknown> | undefined;
        const usageFromMetadata = (md as any)?.usage ?? (md as any)?.tokens ?? undefined;
        const tokensUsed = (md as any)?.tokensUsed as number | undefined;
        const usage =
          usageFromMetadata && typeof usageFromMetadata === 'object'
            ? {
              prompt: (usageFromMetadata as any).prompt_tokens ?? (usageFromMetadata as any).prompt,
              completion: (usageFromMetadata as any).completion_tokens ?? (usageFromMetadata as any).completion,
              total: (usageFromMetadata as any).total_tokens ?? (usageFromMetadata as any).total ?? tokensUsed,
            }
            : tokensUsed
              ? { total: tokensUsed }
              : undefined;

        const finishReason = (md as any)?.finishReason ?? (md as any)?.finish_reason;

        emitTrace({
          trace: config.trace,
          options: config.telemetry,
          ctx: traceContext,
          event: {
            type: 'llm_request_finished',
            level: 'info',
            spanId,
            timing: { startedAt: new Date(startedAt).toISOString(), durationMs: Date.now() - startedAt },
            llm: { provider: providerName, model: config.model, stream: Boolean(config.stream), usage, finishReason },
          },
        });
      }
      return result;
    } catch (error) {
      logger.error(`[ProviderAdapter] Erro no chatCompletion:`, error);
      if (traceContext) {
        emitTrace({
          trace: config.trace,
          options: config.telemetry,
          ctx: traceContext,
          event: {
            type: 'llm_request_failed',
            level: 'error',
            spanId,
            timing: { startedAt: new Date(startedAt).toISOString(), durationMs: Date.now() - startedAt },
            llm: { provider: providerName, model: config.model, stream: Boolean(config.stream) },
            message: (error as Error).message,
          },
        });
      }
      throw error;
    }
  }

  /** 
   * Verifica se um provedor est√° registrado e dispon√≠vel.
   * 
   * @param providerName Nome do provedor a ser verificado.
   * 
   * @returns true se o provedor estiver registrado, false caso contr√°rio.
   * 
   * @example
   * ```typescript
   * if (ProviderAdapter.hasProvider('openai')) {
   *   console.log('OpenAI est√° dispon√≠vel');
   * }
   * 
   * if (ProviderAdapter.hasProvider('anthropic')) {
   *   console.log('Anthropic est√° dispon√≠vel');
   * } else {
   *   console.log('Anthropic n√£o est√° registrado');
   * }
   * ```
   */
  static hasProvider(providerName: string): boolean {
    try {
      getProvider(providerName);
      return true;
    } catch {
      return false;
    }
  }

  /** 
   * Log detalhado das informa√ß√µes do prompt para debugging.
   * 
   * @private
   * @param providerName Nome do provedor sendo usado
   * @param config Configura√ß√£o completa
   */
  private static _logPromptDetails(providerName: string, config: ProviderConfig): void {
    logger.debug('\\n' + '='.repeat(80));
    logger.debug('ü§ñ PROMPT COMPLETO ANTES DA CHAMADA AO LLM');
    logger.debug('='.repeat(80));
    logger.debug(`üìã Provider: ${providerName}`);
    logger.debug(`üéØ Modelo: ${config.model}`);
    logger.debug(`üå°Ô∏è  Temperatura: ${config.temperature || 'default'}`);
    logger.debug(`üî¢ Max Tokens: ${config.maxTokens || 'default'}`);
    logger.debug('='.repeat(80));

    // Log do system prompt completo
    if (config.systemPrompt) {
      logger.debug('\\nüìÑ SYSTEM PROMPT COMPLETO:');
      logger.debug('-'.repeat(60));
      logger.debug(config.systemPrompt);
      logger.debug('-'.repeat(60));
    }

    // Log das mensagens (preview)
    if (config.messages && config.messages.length > 0) {
      logger.debug('\\nüí¨ MENSAGENS DA CONVERSA:');
      logger.debug('-'.repeat(60));
      config.messages.forEach((msg, index) => {
        logger.debug(`${index + 1}. [${msg.role}] ${msg.content}`);
      });
      logger.debug('-'.repeat(60));
    }
  }
}

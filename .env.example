# Copie este arquivo para ".env" na raiz do projeto e preencha suas credenciais.

# =============================================================================
# CONFIGURAÇÕES DE DEBUG
# =============================================================================

# Ativa logs de debug (DEBUG, INFO, WARN, ERROR)
# Valores aceitos: True, true (case-insensitive)
# Padrão: false (apenas WARN e ERROR)
DEBUG=false

# =============================================================================
# CHAVES DE API E CONFIGURAÇÕES DE PROVEDORES LLM
# =============================================================================

# Chave da API OpenAI (usada por OpenAI e provedores compatíveis)
# Obtenha em: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here

# Base URL para provedores compatíveis com OpenAI (obrigatório para openaiCompatible)
# Exemplos:
#   - OpenAI oficial: https://api.openai.com/v1
#   - OpenRouter: https://openrouter.ai/api/v1
#   - Proxy customizado: https://seu-proxy.com/v1
OPENAI_BASE_URL=https://api.openai.com/v1

# Modelo padrão para os testes e execuções
# Exemplos:
#   - OpenAI: gpt-4o-mini, gpt-4, gpt-3.5-turbo
#   - OpenRouter: openai/gpt-4, anthropic/claude-3-opus
#   - Outros: deepseek-chat, llama-3-70b, etc.
OPENAI_MODEL=gpt-4o-mini

# =============================================================================
# VARIÁVEIS COMPATÍVEIS (para código legado react-steps)
# =============================================================================

# As mesmas configurações acima, mas com nomenclatura alternativa
# Mantenha sincronizado com as variáveis OPENAI_* acima
OPENAI_COMPATIBLE_API_KEY=your-openai-api-key-here
OPENAI_COMPATIBLE_BASE_URL=https://api.openai.com/v1
OPENAI_COMPATIBLE_MODEL=gpt-4o-mini

# =============================================================================
# OUTRAS CONFIGURAÇÕES (OPCIONAL)
# =============================================================================

# Limite máximo de tokens para contexto (opcional)
# Se não definido, usa o padrão do modelo
# MAX_CONTEXT_TOKENS=8192

# Temperatura padrão para geração (0.0 a 2.0)
# Valores mais baixos = mais determinístico
# Valores mais altos = mais criativo
# TEMPERATURE=0.7

# Número máximo de tokens na resposta (opcional)
# MAX_TOKENS=2048
